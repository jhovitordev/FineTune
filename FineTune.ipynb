{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "879b6477-1d0c-43a0-9c4c-e52952fa6502",
   "metadata": {},
   "source": [
    "## **Ajuste Fino de um Modelo de IA Generativa para Tarefas de Pergunta e Respostas.**\n",
    "\n",
    "Neste notebook, irei trabalhar com dados extraídos do site oficial da Secretaria de Estado de Desenvolvimento Social de Goiás, com o objetivo de ajustar um modelo pré-treinado para lidar com perguntas e respostas relacionadas aos programas e benefícios oferecidos pela secretária. Para se adequar com a nossa necessidade, o modelo passará por um processo de ajuste fino. Primeiramente, iremos realizar um ajuste fino (Fine Tune, em inglês) total, onde todas as camadas do modelo pré-treinado serão modificadas, e depois apresentaremos o método [PEFT](./PEFT.ipynb) (Ajuste Fino Eficiente de Parâmetros) que gasta menos recursos computacionais, e modifica apenas algumas camadas do modelo, conseguindo assim que ele retenha o conhecimento que já tem e evite o esquecimento catastrófico. Ao final, avaliaremos a precisão dos  dois modelos ajustados e determinaremos sua adequação para as tarefas definidas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6418db8-8344-4055-9abc-8365430be0f5",
   "metadata": {},
   "source": [
    "#### Baixando bibliotecas necessárias:\n",
    "\n",
    "- transformers[torch] : A bibilioteca Transformers pode ser entendida como uma ferramenta poderosa para trabalhar com Processamento de Linguagem Natural. Aqui instalamos o Transformers e a biblioteca de deep learning respectiva apenas numa linha.\n",
    "- Datasets :  A biblioteca Datasets, também desenvolvida pela Hugging Face, é uma coleção de conjuntos de dados prontos para uso em uma variedade de tarefas de processamento de linguagem natural (PLN) e aprendizado de máquina (ML).\n",
    "- Evaluate: Com essa biblioteca você ganha acesso a dezenas de métodos de avaliação para diferentes modelos, o que fdacilita a acilita a avaliação de modelos de machine learning e datasets.\n",
    "- accelerate : A biblioteca accelerate da Hugging Face é uma ferramenta poderosa criada para facilitar o treinamento distribuído de modelos de machine learning, especialmente modelos baseados em Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e25f18-da78-4879-965f-5d727b50cf69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: evaluate in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.41.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (4.66.1)\n",
      "Requirement already satisfied: torch in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (2.2.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers[torch]) (0.30.1)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: xxhash in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2023.4)\n",
      "Requirement already satisfied: psutil in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.8)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.9.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers[torch]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers[torch]) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers[torch]) (2023.11.17)\n",
      "Requirement already satisfied: sympy in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->transformers[torch]) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch->transformers[torch]) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate) (2.2.0)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate) (0.23.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\joao.calmeida\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "#baixando arquivos necessários\n",
    "\n",
    "!pip install transformers[torch] datasets evaluate pandas\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f3391b-3c4b-49d5-a41c-cfa1d9f0f0b3",
   "metadata": {},
   "source": [
    " Mesmo que as bibliotecas já estejam instaladas em seu ambiente Python, vocêdevea importá-las em seu script para poder acessar suas funcionalidades. O comando import é essencial para isso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d47deb33-9858-4276-a8b9-aefebc7e2a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando partes específicas das bibliotecas que serão importantes para o fine-tune\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForPreTraining, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8c306c-78f3-44b9-8e8f-d2648ede0656",
   "metadata": {},
   "source": [
    "#### Escolhendo o modelo\n",
    "\n",
    "Quando vamos realizar um ajuste fino, sempre é interessante escolhermos modelos que foram pré-treinados para tarefas semelhantes. \n",
    "\n",
    "Agora podemos aproveitar para configurar o tokenizador que foi utilizado no treinamento do modelo escolhido. Graças ao **AutoTokenizer** que importamos no inicio do codigo, não precisamos nos preocupar em buscar qual forma de tokenização foi utilizada. Basta utilizar o AutoTokenizer, pois ele faz isso automaticamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4810dcd6-eac7-44cf-bde7-3398d9da8358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Escolhendo o modelo para realizar o Ajuste Fino\n",
    "model_name = \"bigscience/bloom-560m\" \n",
    "model = AutoModelForPreTraining.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a68d24-8b31-4422-838c-cc1ed2dafe4b",
   "metadata": {},
   "source": [
    "#### Preparando os dados\n",
    "\n",
    "Quando vamos realizar um Fine-Tune, antes de qualquer coisa, precisamos preparar os dados que serão utilizados para realizar o ajuste do modelo.\n",
    "\n",
    "Os dados utilizados neste algoritmo foram criados com base nos beneficios oferecidos pela Secretaria de Estado de Desenvolvimento Social de Goiás. Essas informações podem ser consultadas neste [link](\"https://goias.gov.br/social/#\").\n",
    "\n",
    "Para facilitar a utilização e simplificar nosso código, realizamos toda a trativa dos dados em outro notebook. Para acessá-lo, basta clicar neste [link](./Dados/TrabalhandoDados.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3341c6db-1341-43aa-80d4-94809f375cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ea9f4c209b49f58e9951fe9b852a18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joao.calmeida\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\download\\streaming_download_manager.py:784: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d4afb909b74cd698bd6b9b8b850409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['pergunta', 'resposta'],\n",
      "        num_rows: 2120\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['pergunta', 'resposta'],\n",
      "        num_rows: 531\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joao.calmeida\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\download\\streaming_download_manager.py:784: FutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", download_config=download_config), **kwargs)\n"
     ]
    }
   ],
   "source": [
    "data_file = {\"train\": \"./Dados/treino.csv\", \"test\":'./Dados/teste.csv'}\n",
    "dataSetCompleto = load_dataset(\"csv\", data_files=data_file)\n",
    "\n",
    "#atribuindo a variavel dados de tewste e de treino\n",
    "\n",
    "print(dataSetCompleto)\n",
    "\n",
    "treino = dataSetCompleto['train']\n",
    "teste = dataSetCompleto['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864445a0-ca6b-46af-ae89-d78f89a106fd",
   "metadata": {},
   "source": [
    "#### Tokenização\n",
    "\n",
    "Chegamos no momento em que começamos a transformar nossos dados em dados que o modelo escolhido consiga compreender, ou seja, chegamos na fase da Tokenização dos dados.\n",
    "\n",
    "Para facilitar a tokenização dos dados de treinamento e teste, criamos uma função chamada Tokenizer. Essa função receberá os dados a serem tokenizados e retornará os dados tokenizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a023626a-7246-4b94-b926-33c4ba938c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00767053443c4e8dad892ebad4ddae3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/531 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6360a2da8d842af8c87347931edbc2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Aqui criamos uma função para facilitar a tokenização\n",
    "\n",
    "def tokenizar(dados):\n",
    "    return tokenizer(dados[\"pergunta\"], dados[\"resposta\"], truncation=True, padding=\"max_length\", max_length=526)\n",
    "\n",
    "#Utilizamos a função .map() para aplicar a tokenização em todos os elementos dos datasets\n",
    "dataset_teste = teste.map(tokenizar)\n",
    "dataset_treino = treino.map(tokenizar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246eb64b-cfa2-4877-bc67-2ebc16921440",
   "metadata": {},
   "source": [
    "#### DataCollator\n",
    "\n",
    "O DataCollator é um componente utilizado na área de processamento de linguagem natural (NLP), ele é parte integrante do ecossistema de bibliotecas como Transformers da Hugging Face, sua principal função é preparar os dados brutos antes de serem alimentados no modelo durante o treinamento ou a inferência. Ele realiza diversas tarefas essenciais para assegurar que os dados estejam no formato correto e que os modelos possam processá-los de maneira eficiente.\n",
    "\n",
    "Há varios tipos de DataCollator, mas para este Fine Tune iremos usar o *DataCollatorForLanguageModeling* - Coletor de dados usado para modelagem de linguagem. As entradas são preenchidas dinamicamente até o comprimento máximo de um lote se não tiverem todas o mesmo comprimento.\n",
    "\n",
    "Parâmetros: \n",
    "- tokenizer : O tokenizer usado para codificar os dados. \n",
    "- mlm ( bool, opcional , o padrão é True) : Se deve ou não usar modelagem de linguagem mascarada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90658d74-087c-4722-a659-0c9b39321db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A função DataCollatorForLanguageModeling() \n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eeb489-f9df-4db2-b4b6-4b1da7920bfe",
   "metadata": {},
   "source": [
    "#### Treinamento\n",
    "\n",
    "Neste momento precisamos primeiro configurar o \"TrainingArguments\" com as especificações de como você deseja realizar o treinamento. O \"TrainingArguments\" possui diversos parâmetros que oferecem flexibilidade sobre como queremos configurar nosso treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7c72d2-9499-4e71-8f5f-af318adea229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#configurarndo parametros de treino\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/home/seds/projeto_generativo/modeloBloom1',\n",
    "    overwrite_output_dir = True,\n",
    "    evaluation_strategy='epoch',\n",
    "    num_train_epochs=1,\n",
    "    per_device_eval_batch_size=2,\n",
    "    per_device_train_batch_size=3   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d617e463-d65a-4ef0-914a-ebcf46a10066",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_treino,\n",
    "    eval_dataset=dataset_teste,\n",
    "    data_collator=data_collator,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35860b41-f4d8-4339-ad5f-9f90389e6f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec3ad7c-540f-4e9c-9957-7728dde89b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Salvando o modelo ajustado em uma pasta\n",
    "output_model_dir = \"/home/seds/projeto_generativo/modeloBloom1\"\n",
    "model.save_pretrained(output_model_dir)\n",
    "tokenizer.save_pretrained(output_model_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
